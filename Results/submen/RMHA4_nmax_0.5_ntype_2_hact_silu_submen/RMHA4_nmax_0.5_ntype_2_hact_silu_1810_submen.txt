 The dataset SubMen contains 8331 users and 10000 items in total
average sequence length: {2.43}
ItemFeatures DF dimensions (10001, 2048)

#######  Training configuration
norm_type:            	2
max_norm:             	0.5
dataset:              	SubMen
train_dir:            	default
batch_size:           	512
lr:                   	6e-06
std:                  	0.01
maxlen:               	35
hidden_units:         	390
num_heads:            	3
pad_token_id:         	0
num_epochs:           	1000
dropout_rate:         	0.3
cxt_size:             	6
n_workers:            	1
top_k:                	10
test_size:            	10000
validation_point:     	1
print_every_n_point:  	1
exponential_print:    	True
last_items:           	False
reverse:              	True
only_finals:          	True
sampling_mode:        	False
add_users:            	False
mask_user:            	False
user_act:             	silu
user_FF:              	True
loss_type:            	CE
positional_encoding_type: 	
position_concatenation: 	False
RMHA_encoder:         	True
ROPE_encoder:         	False
decoder_head:         	masked
max_relative_position: 	4
normalization_type:   	ln
num_groups:           	3
residual_connection_encoder_FF: 	mul
residual_connection_encoder: 	mul
residual_connection_decoder: 	False
dropout_btn_MHA_FF:   	False
num_encoder_blocks:   	3
num_decoder_blocks:   	1
ln_in_AH_decoder:     	False
ln_in_AH_encoder:     	True
ln_in_Q_decoder:      	False
ln_in_Q_encoder:      	True
layer_norm_eps:       	1e-08
hidden_act:           	silu
hidden_act_out:       	sigmoid
intercalate_act:      	True
mask_before_FF_decoder: 	True
PN:                   	False
embedding_d:          	390
embedding_g:          	1950
intermediate_size:    	1170
hidden_dropout_prob:  	0.3
attention_probs_dropout_prob: 	0.3
saving:               	False
RMHA_decoder:         	False

#######
Loading Configuration ...
Number of steps in the Train dataset: 16272
Number of steps in the Validation dataset: 17
Number of steps in the Test dataset: 17
Loading Model ...
Amount of model parameters 12035791
Loading scheduler and optimizer ...
Evaluation every 16 steps, i.e, 1.0 epochs

#### test Acc: 0, NDCG: 0.1306376317636405 HIT: 0.2894402309035125

#### val Acc: 0, NDCG: 0.4781188605982165 HIT: 0.5749072616906474
Epoch: 1, plus 0 steps train_loss: 0.7359

#### test Acc: 0, NDCG: 0.1235000226693496 HIT: 0.27633701862039783

#### val Acc: 0, NDCG: 0.47946580545132444 HIT: 0.5774348352200592
Epoch: 2, plus 0 steps train_loss: 0.7622

#### test Acc: 0, NDCG: 0.130302077026032 HIT: 0.29215543006771055

#### val Acc: 0, NDCG: 0.47478606578505567 HIT: 0.5660243863732544
Epoch: 3, plus 0 steps train_loss: 0.7475

#### test Acc: 0, NDCG: 0.12712986563003068 HIT: 0.28991714716462125

#### val Acc: 0, NDCG: 0.4808159336109404 HIT: 0.5687032175730004
Epoch: 4, plus 0 steps train_loss: 0.7449

#### test Acc: 0, NDCG: 0.12729902977135982 HIT: 0.2763064364684723

#### val Acc: 0, NDCG: 0.4745437075340721 HIT: 0.565934293006771
Epoch: 5, plus 0 steps train_loss: 0.7531

#### test Acc: 0, NDCG: 0.12301781802086023 HIT: 0.26663834373677525

#### val Acc: 0, NDCG: 0.46602255044029 HIT: 0.5558049883622515
Epoch: 6, plus 0 steps train_loss: 0.7363

#### test Acc: 0, NDCG: 0.1263407995910762 HIT: 0.27189268805543804

#### val Acc: 0, NDCG: 0.4682962003980372 HIT: 0.5607452457151926
Epoch: 7, plus 0 steps train_loss: 0.7277

#### test Acc: 0, NDCG: 0.12783500001568301 HIT: 0.27755865160812526

#### val Acc: 0, NDCG: 0.4684558768183578 HIT: 0.554880084902666
Epoch: 8, plus 0 steps train_loss: 0.7315

#### test Acc: 0, NDCG: 0.13577010248581564 HIT: 0.2846570170334321

#### val Acc: 0, NDCG: 0.4791192496438684 HIT: 0.5666046207151926
Epoch: 9, plus 0 steps train_loss: 0.7226

#### test Acc: 0, NDCG: 0.12878862759610935 HIT: 0.27119756400761746

#### val Acc: 0, NDCG: 0.48544021861367576 HIT: 0.5782812169382142
Epoch: 10, plus 0 steps train_loss: 0.7262

#### test Acc: 0, NDCG: 0.1753522013878604 HIT: 0.33008639044646637

#### val Acc: 0, NDCG: 0.49948976317803223 HIT: 0.5832388317287346
Epoch: 12, plus 0 steps train_loss: 0.7187

#### test Acc: 0, NDCG: 0.23253565296456127 HIT: 0.3799485227994075

#### val Acc: 0, NDCG: 0.5327127512550428 HIT: 0.6189885407850191
Epoch: 14, plus 0 steps train_loss: 0.7194

#### test Acc: 0, NDCG: 0.24812986533853346 HIT: 0.3946965589293271

#### val Acc: 0, NDCG: 0.541557265817825 HIT: 0.6363005184088024
Epoch: 16, plus 0 steps train_loss: 0.7168

#### test Acc: 0, NDCG: 0.3174117287080075 HIT: 0.45783630448582313

#### val Acc: 0, NDCG: 0.5859852005911421 HIT: 0.6753613653195091
Epoch: 18, plus 0 steps train_loss: 0.7118

#### test Acc: 0, NDCG: 0.4487179288692724 HIT: 0.5877269691599661

#### val Acc: 0, NDCG: 0.6625929477918352 HIT: 0.7459342268831993
Epoch: 20, plus 0 steps train_loss: 0.7107

#### test Acc: 0, NDCG: 0.5065174798436036 HIT: 0.6275804723867965

#### val Acc: 0, NDCG: 0.7144031861735661 HIT: 0.7931464570990266
Epoch: 22, plus 0 steps train_loss: 0.7145

#### test Acc: 0, NDCG: 0.47753487414609097 HIT: 0.6103833844688955

#### val Acc: 0, NDCG: 0.6836352327600319 HIT: 0.7618427316969953
Epoch: 24, plus 0 steps train_loss: 0.7094

#### test Acc: 0, NDCG: 0.29547063569395043 HIT: 0.44392886426153194

#### val Acc: 0, NDCG: 0.5827478004883224 HIT: 0.6666909119763013
Epoch: 26, plus 0 steps train_loss: 0.7117

#### test Acc: 0, NDCG: 0.17961756796971348 HIT: 0.3300310119551418

#### val Acc: 0, NDCG: 0.5094192851250019 HIT: 0.6078921789039358
Epoch: 28, plus 0 steps train_loss: 0.7107

#### test Acc: 0, NDCG: 0.3416853349220812 HIT: 0.4910741443609818

#### val Acc: 0, NDCG: 0.6086455201712345 HIT: 0.6909078435780787
Epoch: 30, plus 0 steps train_loss: 0.7113

#### test Acc: 0, NDCG: 0.49980282130823944 HIT: 0.6267092943292425

#### val Acc: 0, NDCG: 0.6985544485514671 HIT: 0.7700114724396954
Epoch: 32, plus 0 steps train_loss: 0.7077

#### test Acc: 0, NDCG: 0.2366339976671571 HIT: 0.38024359923825646

#### val Acc: 0, NDCG: 0.5429884347006528 HIT: 0.6308701200804063
Epoch: 36, plus 0 steps train_loss: 0.7067

#### test Acc: 0, NDCG: 0.21949182313631838 HIT: 0.3682719265763859

#### val Acc: 0, NDCG: 0.5444416011491591 HIT: 0.6358393064959796
Epoch: 40, plus 0 steps train_loss: 0.7072

#### test Acc: 0, NDCG: 0.26584323785545805 HIT: 0.41012070858019467

#### val Acc: 0, NDCG: 0.5514303981875613 HIT: 0.6400786539885738
Epoch: 44, plus 0 steps train_loss: 0.6999

#### test Acc: 0, NDCG: 0.1332172804724584 HIT: 0.28434871588023697

#### val Acc: 0, NDCG: 0.49422372283854266 HIT: 0.5900536923402455
Epoch: 48, plus 0 steps train_loss: 0.7068

#### test Acc: 0, NDCG: 0.5701827722663874 HIT: 0.6821398579665678

#### val Acc: 0, NDCG: 0.7329802699758255 HIT: 0.801073846804909
Epoch: 52, plus 0 steps train_loss: 0.7006

#### test Acc: 0, NDCG: 0.5334826170426832 HIT: 0.6628995516821836

#### val Acc: 0, NDCG: 0.7200863298840151 HIT: 0.7905883014176894
Epoch: 56, plus 0 steps train_loss: 0.7038

#### test Acc: 0, NDCG: 0.5406560589768377 HIT: 0.6561937949640287

#### val Acc: 0, NDCG: 0.7220573365523318 HIT: 0.7939986246297079
Epoch: 60, plus 0 steps train_loss: 0.6995

#### test Acc: 0, NDCG: 0.5397650846618652 HIT: 0.6613754033537875

#### val Acc: 0, NDCG: 0.7291674968504823 HIT: 0.7977056773698687
Epoch: 64, plus 0 steps train_loss: 0.7031

#### test Acc: 0, NDCG: 0.5181975289924312 HIT: 0.6510774836013542

#### val Acc: 0, NDCG: 0.7125539807913777 HIT: 0.7849049804274228
Epoch: 68, plus 0 steps train_loss: 0.7035

#### test Acc: 0, NDCG: 0.428185453514493 HIT: 0.5574746085484553

#### val Acc: 0, NDCG: 0.6696473769643033 HIT: 0.7527854554591621
Epoch: 72, plus 0 steps train_loss: 0.7008

#### test Acc: 0, NDCG: 0.30895812995540345 HIT: 0.4532159199111299

#### val Acc: 0, NDCG: 0.5860403406836975 HIT: 0.6786989526026238
Epoch: 80, plus 0 steps train_loss: 0.6983

#### test Acc: 0, NDCG: 0.4854804408511579 HIT: 0.6199729554591621

#### val Acc: 0, NDCG: 0.6680333131375659 HIT: 0.7447431760473974
Epoch: 88, plus 0 steps train_loss: 0.7007

#### test Acc: 0, NDCG: 0.20890358232620457 HIT: 0.3647524994710114

#### val Acc: 0, NDCG: 0.5445124292065306 HIT: 0.642242547873466
Epoch: 96, plus 0 steps train_loss: 0.6983

#### test Acc: 0, NDCG: 0.3812461451542208 HIT: 0.5293629985717309

#### val Acc: 0, NDCG: 0.6384291481354195 HIT: 0.7213172476724502
Epoch: 104, plus 0 steps train_loss: 0.6986

#### test Acc: 0, NDCG: 0.4648898902234187 HIT: 0.5961370609394837

#### val Acc: 0, NDCG: 0.6960007024653716 HIT: 0.7710454797926365
Epoch: 112, plus 0 steps train_loss: 0.6961

#### test Acc: 0, NDCG: 0.5405680914366577 HIT: 0.6565632604210749

#### val Acc: 0, NDCG: 0.7232666156083206 HIT: 0.7965452086859923
Epoch: 120, plus 0 steps train_loss: 0.6969

#### test Acc: 0, NDCG: 0.5544241628609791 HIT: 0.6715948013647906

#### val Acc: 0, NDCG: 0.7285713306851699 HIT: 0.7991992435463393
Epoch: 128, plus 0 steps train_loss: 0.6934

#### test Acc: 0, NDCG: 0.598060064667771 HIT: 0.7088000555438002

#### val Acc: 0, NDCG: 0.7554278545715435 HIT: 0.8197587150867541
Epoch: 136, plus 0 steps train_loss: 0.696

#### test Acc: 0, NDCG: 0.521088039405225 HIT: 0.6498558506136267

#### val Acc: 0, NDCG: 0.7085163205418008 HIT: 0.7836643369128227
Epoch: 144, plus 0 steps train_loss: 0.6989

#### test Acc: 0, NDCG: 0.47928813074199944 HIT: 0.6156319429750318

#### val Acc: 0, NDCG: 0.6776320730555434 HIT: 0.7525250938954718
Epoch: 160, plus 0 steps train_loss: 0.694

#### test Acc: 0, NDCG: 0.5615429468431378 HIT: 0.683700374259416

#### val Acc: 0, NDCG: 0.7318570773812586 HIT: 0.8073564126639864
Epoch: 176, plus 0 steps train_loss: 0.6901

#### test Acc: 0, NDCG: 0.40743315224779164 HIT: 0.54598563796022

#### val Acc: 0, NDCG: 0.6417421290401493 HIT: 0.7243523196148963
Epoch: 192, plus 0 steps train_loss: 0.6879

#### test Acc: 0, NDCG: 0.29438357951038463 HIT: 0.44291800015869653

#### val Acc: 0, NDCG: 0.5762678691383559 HIT: 0.6635219398011003
Epoch: 208, plus 0 steps train_loss: 0.6909

#### test Acc: 0, NDCG: 0.29847804601514444 HIT: 0.443057686203978

#### val Acc: 0, NDCG: 0.5826622623151059 HIT: 0.6713765935780787
Epoch: 224, plus 0 steps train_loss: 0.6793

#### test Acc: 0, NDCG: 0.30113187504413874 HIT: 0.4417401740372408

#### val Acc: 0, NDCG: 0.5729303713026974 HIT: 0.6602992422238679
Epoch: 240, plus 0 steps train_loss: 0.6859

#### test Acc: 0, NDCG: 0.24594191164205934 HIT: 0.39603142853364365

#### val Acc: 0, NDCG: 0.5582812510662005 HIT: 0.6523966488573847
Epoch: 256, plus 0 steps train_loss: 0.6762

#### test Acc: 0, NDCG: 0.20693846148490302 HIT: 0.3595403089293271

#### val Acc: 0, NDCG: 0.521005451413624 HIT: 0.6203366351036818
Epoch: 272, plus 0 steps train_loss: 0.6627

#### test Acc: 0, NDCG: 0.16295849468108783 HIT: 0.31969837732754974

#### val Acc: 0, NDCG: 0.4998120030035526 HIT: 0.602601466620821
Epoch: 288, plus 0 steps train_loss: 0.6668

#### test Acc: 0, NDCG: 0.16477791758074836 HIT: 0.3391130845323741

#### val Acc: 0, NDCG: 0.49813814811703155 HIT: 0.5993118189272112
Epoch: 304, plus 0 steps train_loss: 0.6736

#### test Acc: 0, NDCG: 0.15675868012276287 HIT: 0.3192867580935252

#### val Acc: 0, NDCG: 0.5056679281251428 HIT: 0.6075111418218366
Epoch: 320, plus 0 steps train_loss: 0.6629

#### test Acc: 0, NDCG: 0.17293816548876217 HIT: 0.349502750740584

#### val Acc: 0, NDCG: 0.5107050666279609 HIT: 0.6187951293377063
Epoch: 352, plus 0 steps train_loss: 0.6569

#### test Acc: 0, NDCG: 0.19165612802455112 HIT: 0.37827890261320357

#### val Acc: 0, NDCG: 0.52281750414008 HIT: 0.6291699177422768
Epoch: 384, plus 0 steps train_loss: 0.6313

#### test Acc: 0, NDCG: 0.2111787645273792 HIT: 0.3943940435886585

#### val Acc: 0, NDCG: 0.5251733977984491 HIT: 0.6297327946466357
Epoch: 416, plus 0 steps train_loss: 0.6148

#### test Acc: 0, NDCG: 0.21383828527909607 HIT: 0.39193920598815063

#### val Acc: 0, NDCG: 0.5317227558810246 HIT: 0.6360037888806601
Epoch: 448, plus 0 steps train_loss: 0.5798

#### test Acc: 0, NDCG: 0.23136220918097225 HIT: 0.4134888780152349

#### val Acc: 0, NDCG: 0.5373862963460236 HIT: 0.6420681469530258
Epoch: 480, plus 0 steps train_loss: 0.5687

#### test Acc: 0, NDCG: 0.2362932457023336 HIT: 0.418846540414727

#### val Acc: 0, NDCG: 0.5438099313050244 HIT: 0.6470927118599238
Epoch: 512, plus 0 steps train_loss: 0.5674

#### test Acc: 0, NDCG: 0.24887760786482307 HIT: 0.4411103470165044

#### val Acc: 0, NDCG: 0.5431923094743153 HIT: 0.6463686587494709
Epoch: 544, plus 0 steps train_loss: 0.5666

#### test Acc: 0, NDCG: 0.2550131235410611 HIT: 0.44484798190859076

#### val Acc: 0, NDCG: 0.5493170005023635 HIT: 0.6561020485082523
Epoch: 576, plus 0 steps train_loss: 0.5736

#### test Acc: 0, NDCG: 0.2531356413234141 HIT: 0.44290064272111723

#### val Acc: 0, NDCG: 0.551677342368946 HIT: 0.6541009839187474
Epoch: 608, plus 0 steps train_loss: 0.5733

#### test Acc: 0, NDCG: 0.26281676341424776 HIT: 0.4488691216144731

#### val Acc: 0, NDCG: 0.5485130662084743 HIT: 0.6479829004443504
Epoch: 640, plus 0 steps train_loss: 0.536

#### test Acc: 0, NDCG: 0.26429245427174564 HIT: 0.4547342824269996

#### val Acc: 0, NDCG: 0.5445221130343133 HIT: 0.6462653406686416
Epoch: 704, plus 0 steps train_loss: 0.5681

#### test Acc: 0, NDCG: 0.26965970231015446 HIT: 0.4561245305226407

#### val Acc: 0, NDCG: 0.5427372014130136 HIT: 0.6423417332310623
Epoch: 768, plus 0 steps train_loss: 0.5533

#### test Acc: 0, NDCG: 0.2626324005276565 HIT: 0.44990891477994077

#### val Acc: 0, NDCG: 0.5412432848170293 HIT: 0.6506005673402455
Epoch: 832, plus 0 steps train_loss: 0.5349

#### test Acc: 0, NDCG: 0.2688623713287226 HIT: 0.4590389269466779

#### val Acc: 0, NDCG: 0.5576397694947081 HIT: 0.6588784119763013
Epoch: 896, plus 0 steps train_loss: 0.5326

#### test Acc: 0, NDCG: 0.27305196766957623 HIT: 0.4642643422027084

#### val Acc: 0, NDCG: 0.5535882960330651 HIT: 0.6586296220376641
Epoch: 960, plus 0 steps train_loss: 0.5227

#### test Acc: 0, NDCG: 0.2643929993931816 HIT: 0.45164383199322894

#### val Acc: 0, NDCG: 0.5471150313659392 HIT: 0.6510791366906474
Epoch: 1017, plus 0 steps train_loss: 0.5153
Done: it took 141226.87003421783
max value of NDCG: 0.598060064667771
max value of HIT: 0.7088000555438002

After 20 validations
max value of NDCG: 0.598060064667771
max value of HIT: 0.7088000555438002
